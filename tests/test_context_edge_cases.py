#!/usr/bin/env python3
"""
ä¸Šä¸‹æ–‡ç®¡ç†è¾¹ç•Œæµ‹è¯•

æµ‹è¯•å„ç§è¾¹ç•Œæƒ…å†µä¸‹çš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼š
1. çº¯æ–‡æœ¬è¾“å‡ºï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰
2. å¤šè½®å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬è¾“å‡º
3. æ··åˆåœºæ™¯æµ‹è¯•
"""

import asyncio
import os
import sys
from typing import Dict, Any
from pydantic import BaseModel

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from simple_agent_lib import LLMAPIClient, Agent, Context, tool
from simple_agent_lib.schemas import (
    ReasoningChunkEvent,
    ContentChunkEvent,
    ToolCallCompleteEvent,
    AllToolResultsEvent,
    LLMEndReasonEvent,
)


# æµ‹è¯•ç”¨å·¥å…·
class WeatherResponse(BaseModel):
    """å¤©æ°”å“åº”æ¨¡å‹"""
    city: str
    temperature: float
    weather: str


@tool
def get_weather(city: str) -> WeatherResponse:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
    city_weather_map = {
        "åŒ—äº¬": {"temperature": 25.0, "weather": "æ™´å¤©"},
        "ä¸Šæµ·": {"temperature": 22.0, "weather": "å¤šäº‘"},
        "å¹¿å·": {"temperature": 28.0, "weather": "å°é›¨"},
        "æ·±åœ³": {"temperature": 27.0, "weather": "é˜´å¤©"},
    }
    
    weather_info = city_weather_map.get(city, {"temperature": 20.0, "weather": "æ™´å¤©"})
    return WeatherResponse(
        city=city,
        temperature=weather_info["temperature"],
        weather=weather_info["weather"],
    )


@tool
def calculate_sum(a: float, b: float) -> Dict[str, Any]:
    """è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ"""
    result = a + b
    return {
        "a": a,
        "b": b,
        "sum": result,
        "operation": f"{a} + {b} = {result}"
    }


@tool
def get_time() -> Dict[str, str]:
    """è·å–å½“å‰æ—¶é—´ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    return {
        "current_time": "2025-07-01 12:00:00",
        "timezone": "UTC+8",
        "day_of_week": "æ˜ŸæœŸä¸€"
    }


async def create_test_agent() -> Agent:
    """åˆ›å»ºæµ‹è¯•ç”¨çš„Agent"""
    api_base = os.getenv("OPENAI_API_BASE", "http://localhost:8000/v1")
    api_key = os.getenv("OPENAI_API_KEY", "test-key")
    model_name = os.getenv("LLM_MODEL_NAME", "gemini-2.5-flash")
    
    llm_client = LLMAPIClient(
        base_url=api_base,
        api_key=api_key,
        model_name=model_name
    )
    
    agent = Agent(
        llm_api_client=llm_client,
        tools=[get_weather, calculate_sum, get_time],
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ã€‚",
        debug_mode=True
    )
    
    return agent


async def test_pure_text_response(stream_mode: bool) -> list[dict]:
    """æµ‹è¯•çº¯æ–‡æœ¬å“åº”ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰"""
    mode_name = "æµå¼" if stream_mode else "éæµå¼"
    print(f"\n--- æµ‹è¯•çº¯æ–‡æœ¬å“åº”ï¼ˆ{mode_name}æ¨¡å¼ï¼‰---")
    
    agent = await create_test_agent()
    
    # ä½¿ç”¨ä¸€ä¸ªä¸ä¼šè§¦å‘å·¥å…·è°ƒç”¨çš„ç®€å•é—®é¢˜
    prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹ã€‚"
    print(f"ç”¨æˆ·è¾“å…¥: {prompt}")
    
    content_chunks = []
    async for event in agent.run(initial_prompt=prompt, stream_mode=stream_mode):
        if isinstance(event, ContentChunkEvent):
            content_chunks.append(event.text)
            print(f"å†…å®¹: {event.text}", end="")
        elif isinstance(event, ToolCallCompleteEvent):
            print(f"\nâš ï¸  æ„å¤–çš„å·¥å…·è°ƒç”¨: {event.tool_call.name}")
        elif isinstance(event, LLMEndReasonEvent):
            print(f"\nç»“æŸåŸå› : {event.finish_reason}")
    
    print(f"\næ”¶é›†åˆ°çš„å†…å®¹å—æ•°é‡: {len(content_chunks)}")
    print(f"å®Œæ•´å†…å®¹: {''.join(content_chunks)}")
    
    context_messages = agent.context.to_openai_messages()
    print(f"æœ€ç»ˆä¸Šä¸‹æ–‡æ¶ˆæ¯æ•°é‡: {len(context_messages)}")
    
    return context_messages


async def test_multiple_tools_and_text(stream_mode: bool) -> list[dict]:
    """æµ‹è¯•å¤šè½®å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬è¾“å‡º"""
    mode_name = "æµå¼" if stream_mode else "éæµå¼"
    print(f"\n--- æµ‹è¯•å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆ{mode_name}æ¨¡å¼ï¼‰---")
    
    agent = await create_test_agent()
    
    # ä½¿ç”¨ä¸€ä¸ªå¯èƒ½è§¦å‘å¤šæ¬¡å·¥å…·è°ƒç”¨çš„å¤æ‚é—®é¢˜
    prompt = "è¯·å‘Šè¯‰æˆ‘åŒ—äº¬å’Œä¸Šæµ·çš„å¤©æ°”ï¼Œç„¶åè®¡ç®—ä¸¤ä¸ªåŸå¸‚æ¸©åº¦çš„å’Œï¼Œæœ€åå‘Šè¯‰æˆ‘ç°åœ¨çš„æ—¶é—´ã€‚è¯·è¯¦ç»†è¯´æ˜æ¯ä¸€æ­¥çš„ç»“æœã€‚"
    print(f"ç”¨æˆ·è¾“å…¥: {prompt}")
    
    tool_calls_count = 0
    content_chunks = []
    
    async for event in agent.run(initial_prompt=prompt, stream_mode=stream_mode):
        if isinstance(event, ContentChunkEvent):
            content_chunks.append(event.text)
            print(f"å†…å®¹: {event.text}", end="")
        elif isinstance(event, ToolCallCompleteEvent):
            tool_calls_count += 1
            print(f"\nğŸ”§ å·¥å…·è°ƒç”¨ #{tool_calls_count}: {event.tool_call.name}({event.tool_call.args})")
        elif isinstance(event, AllToolResultsEvent):
            print(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»“æœæ•°é‡: {len(event.results)}")
        elif isinstance(event, LLMEndReasonEvent):
            print(f"\nğŸ ç»“æŸåŸå› : {event.finish_reason}")
    
    print(f"\næ€»å·¥å…·è°ƒç”¨æ¬¡æ•°: {tool_calls_count}")
    print(f"æ”¶é›†åˆ°çš„å†…å®¹å—æ•°é‡: {len(content_chunks)}")
    
    context_messages = agent.context.to_openai_messages()
    print(f"æœ€ç»ˆä¸Šä¸‹æ–‡æ¶ˆæ¯æ•°é‡: {len(context_messages)}")
    
    return context_messages


def analyze_context_completeness(context_messages: list[dict], test_name: str) -> bool:
    """åˆ†æä¸Šä¸‹æ–‡çš„å®Œæ•´æ€§"""
    print(f"\n=== {test_name} ä¸Šä¸‹æ–‡åˆ†æ ===")
    
    assistant_messages = [msg for msg in context_messages if msg.get('role') == 'assistant']
    tool_messages = [msg for msg in context_messages if msg.get('role') == 'tool']
    
    print(f"åŠ©æ‰‹æ¶ˆæ¯æ•°é‡: {len(assistant_messages)}")
    print(f"å·¥å…·æ¶ˆæ¯æ•°é‡: {len(tool_messages)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åŠ©æ‰‹æ–‡æœ¬å†…å®¹
    text_responses = [msg for msg in assistant_messages if msg.get('content')]
    tool_call_requests = [msg for msg in assistant_messages if msg.get('tool_calls')]
    
    print(f"åŒ…å«æ–‡æœ¬å†…å®¹çš„åŠ©æ‰‹æ¶ˆæ¯: {len(text_responses)}")
    print(f"åŒ…å«å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯: {len(tool_call_requests)}")
    
    # è¯¦ç»†åˆ—å‡ºæ‰€æœ‰æ¶ˆæ¯
    for i, msg in enumerate(context_messages, 1):
        role = msg.get('role', 'æœªçŸ¥')
        content = (msg.get('content', '') or '')[:50] + "..." if len((msg.get('content', '') or '')) > 50 else (msg.get('content', '') or '')
        tool_calls_count = len(msg.get('tool_calls', []))
        tool_call_id = msg.get('tool_call_id', '')
        
        print(f"  {i}. {role}: '{content}' (å·¥å…·è°ƒç”¨: {tool_calls_count}) (tool_call_id: {tool_call_id})")
    
    return True


async def test_context_edge_cases():
    """ä¸»è¦è¾¹ç•Œæµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹ä¸Šä¸‹æ–‡ç®¡ç†è¾¹ç•Œæµ‹è¯•...")
    
    results = []
    
    # æµ‹è¯•1: çº¯æ–‡æœ¬å“åº” - æµå¼
    try:
        stream_context = await test_pure_text_response(stream_mode=True)
        analyze_context_completeness(stream_context, "çº¯æ–‡æœ¬å“åº”ï¼ˆæµå¼ï¼‰")
        results.append(("çº¯æ–‡æœ¬å“åº”ï¼ˆæµå¼ï¼‰", True, len(stream_context)))
    except Exception as e:
        print(f"âŒ çº¯æ–‡æœ¬å“åº”ï¼ˆæµå¼ï¼‰æµ‹è¯•å¤±è´¥: {e}")
        results.append(("çº¯æ–‡æœ¬å“åº”ï¼ˆæµå¼ï¼‰", False, 0))
    
    # æµ‹è¯•2: çº¯æ–‡æœ¬å“åº” - éæµå¼
    try:
        non_stream_context = await test_pure_text_response(stream_mode=False)
        analyze_context_completeness(non_stream_context, "çº¯æ–‡æœ¬å“åº”ï¼ˆéæµå¼ï¼‰")
        results.append(("çº¯æ–‡æœ¬å“åº”ï¼ˆéæµå¼ï¼‰", True, len(non_stream_context)))
    except Exception as e:
        print(f"âŒ çº¯æ–‡æœ¬å“åº”ï¼ˆéæµå¼ï¼‰æµ‹è¯•å¤±è´¥: {e}")
        results.append(("çº¯æ–‡æœ¬å“åº”ï¼ˆéæµå¼ï¼‰", False, 0))
    
    # æµ‹è¯•3: å¤šè½®å·¥å…·è°ƒç”¨ - æµå¼
    try:
        multi_stream_context = await test_multiple_tools_and_text(stream_mode=True)
        analyze_context_completeness(multi_stream_context, "å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼‰")
        results.append(("å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼‰", True, len(multi_stream_context)))
    except Exception as e:
        print(f"âŒ å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼‰æµ‹è¯•å¤±è´¥: {e}")
        results.append(("å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼‰", False, 0))
    
    # æµ‹è¯•4: å¤šè½®å·¥å…·è°ƒç”¨ - éæµå¼
    try:
        multi_non_stream_context = await test_multiple_tools_and_text(stream_mode=False)
        analyze_context_completeness(multi_non_stream_context, "å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰")
        results.append(("å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰", True, len(multi_non_stream_context)))
    except Exception as e:
        print(f"âŒ å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰æµ‹è¯•å¤±è´¥: {e}")
        results.append(("å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰", False, 0))
    
    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š è¾¹ç•Œæµ‹è¯•ç»“æœæ±‡æ€»:")
    passed = 0
    for test_name, success, msg_count in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status} (æ¶ˆæ¯æ•°: {msg_count})")
        if success:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{len(results)} é€šè¿‡")
    
    # æ£€æŸ¥å…³é”®æœŸæœ›
    print(f"\nğŸ” å…³é”®æ£€æŸ¥:")
    
    # 1. çº¯æ–‡æœ¬æ¨¡å¼ä¸‹åº”è¯¥æœ‰åŠ©æ‰‹å›å¤
    pure_text_tests = [r for r in results if "çº¯æ–‡æœ¬" in r[0] and r[1]]
    if pure_text_tests:
        print(f"âœ… çº¯æ–‡æœ¬å“åº”æµ‹è¯•: {len(pure_text_tests)}/2 é€šè¿‡")
        for name, _, count in pure_text_tests:
            if count >= 2:  # è‡³å°‘ç³»ç»Ÿæ¶ˆæ¯ + ç”¨æˆ·æ¶ˆæ¯ + åŠ©æ‰‹å›å¤
                print(f"  âœ… {name}: ä¸Šä¸‹æ–‡åŒ…å«åŠ©æ‰‹å›å¤ (æ¶ˆæ¯æ•°: {count})")
            else:
                print(f"  âš ï¸  {name}: ä¸Šä¸‹æ–‡å¯èƒ½ç¼ºå°‘åŠ©æ‰‹å›å¤ (æ¶ˆæ¯æ•°: {count})")
    
    # 2. å¤šè½®å·¥å…·è°ƒç”¨åº”è¯¥æœ‰å®Œæ•´çš„å¯¹è¯æµ
    multi_tool_tests = [r for r in results if "å¤šè½®" in r[0] and r[1]]
    if multi_tool_tests:
        print(f"âœ… å¤šè½®å·¥å…·è°ƒç”¨æµ‹è¯•: {len(multi_tool_tests)}/2 é€šè¿‡")
        for name, _, count in multi_tool_tests:
            if count >= 6:  # ç³»ç»Ÿ+ç”¨æˆ·+å¤šä¸ªåŠ©æ‰‹+å·¥å…·æ¶ˆæ¯
                print(f"  âœ… {name}: ä¸Šä¸‹æ–‡åŒ…å«å®Œæ•´å¯¹è¯æµ (æ¶ˆæ¯æ•°: {count})")
            else:
                print(f"  âš ï¸  {name}: ä¸Šä¸‹æ–‡å¯èƒ½ä¸å®Œæ•´ (æ¶ˆæ¯æ•°: {count})")
    
    return passed == len(results)


def main():
    """ä¸»å‡½æ•°"""
    return asyncio.run(test_context_edge_cases())


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 