#!/usr/bin/env python3
"""
ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æµ‹è¯•

æµ‹è¯•æµå¼æ¨¡å¼å’Œéæµå¼æ¨¡å¼ä¸‹Agentçš„ä¸Šä¸‹æ–‡ç®¡ç†æ˜¯å¦ä¸€è‡´
"""

import asyncio
import os
import sys
import random
from typing import Dict, Any
from pydantic import BaseModel
import pytest

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä½¿ç”¨å½“å‰é¡¹ç›®çš„ä»£ç è€Œä¸æ˜¯å·²å®‰è£…çš„ç‰ˆæœ¬
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from simple_agent_lib import LLMAPIClient, Agent, Context, tool
from simple_agent_lib.schemas import (
    ReasoningChunkEvent,
    ContentChunkEvent,
    ToolCallCompleteEvent,
    AllToolResultsEvent,
    LLMEndReasonEvent,
)


# æµ‹è¯•ç”¨çš„ç®€å•å·¥å…·
class WeatherResponse(BaseModel):
    """å¤©æ°”å“åº”æ¨¡å‹"""
    city: str
    temperature: float
    weather: str


@tool
def get_weather(city: str) -> WeatherResponse:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯
    
    Args:
        city: åŸå¸‚åç§°
        
    Returns:
        WeatherResponse: å¤©æ°”ä¿¡æ¯
    """
    # æ¨¡æ‹Ÿè·å–å¤©æ°”ä¿¡æ¯ï¼Œä¸ºäº†æµ‹è¯•ä¸€è‡´æ€§ä½¿ç”¨å›ºå®šå€¼
    city_weather_map = {
        "åŒ—äº¬": {"temperature": 25.0, "weather": "æ™´å¤©"},
        "ä¸Šæµ·": {"temperature": 22.0, "weather": "å¤šäº‘"},
        "å¹¿å·": {"temperature": 28.0, "weather": "å°é›¨"},
        "æ·±åœ³": {"temperature": 27.0, "weather": "é˜´å¤©"},
    }
    
    weather_info = city_weather_map.get(city, {"temperature": 20.0, "weather": "æ™´å¤©"})
    return WeatherResponse(
        city=city,
        temperature=weather_info["temperature"],
        weather=weather_info["weather"],
    )


@tool
def calculate_sum(a: float, b: float) -> Dict[str, Any]:
    """è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ
    
    Args:
        a: ç¬¬ä¸€ä¸ªæ•°
        b: ç¬¬äºŒä¸ªæ•°
        
    Returns:
        Dict[str, Any]: è®¡ç®—ç»“æœ
    """
    result = a + b
    return {
        "a": a,
        "b": b,
        "sum": result,
        "operation": f"{a} + {b} = {result}"
    }


async def create_test_agent() -> Agent:
    """åˆ›å»ºæµ‹è¯•ç”¨çš„Agent"""
    # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
    api_base = os.getenv("OPENAI_API_BASE", "http://localhost:8000/v1")
    api_key = os.getenv("OPENAI_API_KEY", "test-key")
    model_name = os.getenv("LLM_MODEL_NAME", "gemini-2.5-flash")
    
    llm_client = LLMAPIClient(
        base_url=api_base,
        api_key=api_key,
        model_name=model_name
    )
    
    agent = Agent(
        llm_api_client=llm_client,
        tools=[get_weather, calculate_sum],
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·è¿›è¡Œæµ‹è¯•ã€‚",
        debug_mode=True
    )
    
    return agent


async def run_conversation_stream(agent: Agent, prompts: list[str]) -> list[dict]:
    """è¿è¡Œæµå¼å¯¹è¯å¹¶æ”¶é›†ä¸Šä¸‹æ–‡"""
    collected_events = []
    
    for i, prompt in enumerate(prompts):
        print(f"\n--- æµå¼æ¨¡å¼ - ç¬¬{i+1}è½®å¯¹è¯ ---")
        print(f"ç”¨æˆ·è¾“å…¥: {prompt}")
        
        # ç¬¬ä¸€è½®ä½¿ç”¨runï¼Œåç»­ä½¿ç”¨continue_run
        if i == 0:
            async for event in agent.run(initial_prompt=prompt, stream_mode=True):
                collected_events.append(event)
                if isinstance(event, ContentChunkEvent):
                    print(f"å†…å®¹: {event.text}", end="")
                elif isinstance(event, ToolCallCompleteEvent):
                    print(f"\nå·¥å…·è°ƒç”¨: {event.tool_call.name}({event.tool_call.args})")
                elif isinstance(event, AllToolResultsEvent):
                    print(f"å·¥å…·ç»“æœæ•°é‡: {len(event.results)}")
        else:
            async for event in agent.continue_run(prompt=prompt, stream_mode=True):
                collected_events.append(event)
                if isinstance(event, ContentChunkEvent):
                    print(f"å†…å®¹: {event.text}", end="")
                elif isinstance(event, ToolCallCompleteEvent):
                    print(f"\nå·¥å…·è°ƒç”¨: {event.tool_call.name}({event.tool_call.args})")
                elif isinstance(event, AllToolResultsEvent):
                    print(f"å·¥å…·ç»“æœæ•°é‡: {len(event.results)}")
    
    # è¿”å›æœ€ç»ˆçš„ä¸Šä¸‹æ–‡æ¶ˆæ¯
    return agent.context.to_openai_messages()


async def run_conversation_non_stream(agent: Agent, prompts: list[str]) -> list[dict]:
    """è¿è¡Œéæµå¼å¯¹è¯å¹¶æ”¶é›†ä¸Šä¸‹æ–‡"""
    
    for i, prompt in enumerate(prompts):
        print(f"\n--- éæµå¼æ¨¡å¼ - ç¬¬{i+1}è½®å¯¹è¯ ---")
        print(f"ç”¨æˆ·è¾“å…¥: {prompt}")
        
        # ç¬¬ä¸€è½®ä½¿ç”¨runï¼Œåç»­ä½¿ç”¨continue_run
        if i == 0:
            async for event in agent.run(initial_prompt=prompt, stream_mode=False):
                if isinstance(event, ContentChunkEvent):
                    print(f"å†…å®¹: {event.text}")
                elif isinstance(event, ToolCallCompleteEvent):
                    print(f"å·¥å…·è°ƒç”¨: {event.tool_call.name}({event.tool_call.args})")
                elif isinstance(event, AllToolResultsEvent):
                    print(f"å·¥å…·ç»“æœæ•°é‡: {len(event.results)}")
        else:
            async for event in agent.continue_run(prompt=prompt, stream_mode=False):
                if isinstance(event, ContentChunkEvent):
                    print(f"å†…å®¹: {event.text}")
                elif isinstance(event, ToolCallCompleteEvent):
                    print(f"å·¥å…·è°ƒç”¨: {event.tool_call.name}({event.tool_call.args})")
                elif isinstance(event, AllToolResultsEvent):
                    print(f"å·¥å…·ç»“æœæ•°é‡: {len(event.results)}")
    
    # è¿”å›æœ€ç»ˆçš„ä¸Šä¸‹æ–‡æ¶ˆæ¯
    return agent.context.to_openai_messages()


def normalize_message_for_comparison(msg: dict) -> dict:
    """æ ‡å‡†åŒ–æ¶ˆæ¯ä»¥ä¾¿æ¯”è¾ƒï¼ˆç§»é™¤æ—¶é—´æˆ³ç­‰åŠ¨æ€å­—æ®µï¼‰"""
    normalized = msg.copy()
    
    # ç§»é™¤å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´çš„å­—æ®µ
    if 'timestamp' in normalized:
        del normalized['timestamp']
    if 'message_id' in normalized:
        del normalized['message_id']
    
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ ‡å‡†åŒ–å·¥å…·è°ƒç”¨IDï¼ˆå®ƒä»¬å¯èƒ½ä¸åŒä½†ç»“æ„åº”è¯¥ç›¸åŒï¼‰
    if 'tool_calls' in normalized and normalized['tool_calls']:
        for i, tool_call in enumerate(normalized['tool_calls']):
            # å°†tool_call_idæ ‡å‡†åŒ–ä¸ºç´¢å¼•
            tool_call['id'] = f"call_{i}"
    
    # å¦‚æœæ˜¯å·¥å…·å“åº”æ¶ˆæ¯ï¼Œä¹Ÿæ ‡å‡†åŒ–tool_call_id
    if normalized.get('role') == 'tool' and 'tool_call_id' in normalized:
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ ¹æ®æ¶ˆæ¯çš„é¡ºåºæ¥æ ‡å‡†åŒ–ï¼Œæš‚æ—¶ä¿æŒåŸæ ·
        pass
    
    return normalized


def compare_contexts(stream_context: list[dict], non_stream_context: list[dict]) -> tuple[bool, str]:
    """æ¯”è¾ƒä¸¤ä¸ªä¸Šä¸‹æ–‡çš„å†…å®¹æ˜¯å¦ä¸€è‡´"""
    
    if len(stream_context) != len(non_stream_context):
        return False, f"æ¶ˆæ¯æ•°é‡ä¸ä¸€è‡´: æµå¼={len(stream_context)}, éæµå¼={len(non_stream_context)}"
    
    for i, (stream_msg, non_stream_msg) in enumerate(zip(stream_context, non_stream_context)):
        # æ ‡å‡†åŒ–æ¶ˆæ¯
        norm_stream = normalize_message_for_comparison(stream_msg)
        norm_non_stream = normalize_message_for_comparison(non_stream_msg)
        
        # æ¯”è¾ƒå…³é”®å­—æ®µ
        if norm_stream.get('role') != norm_non_stream.get('role'):
            return False, f"ç¬¬{i+1}æ¡æ¶ˆæ¯è§’è‰²ä¸ä¸€è‡´: æµå¼={norm_stream.get('role')}, éæµå¼={norm_non_stream.get('role')}"
        
        if norm_stream.get('content') != norm_non_stream.get('content'):
            return False, f"ç¬¬{i+1}æ¡æ¶ˆæ¯å†…å®¹ä¸ä¸€è‡´: æµå¼='{norm_stream.get('content')}', éæµå¼='{norm_non_stream.get('content')}'"
        
        # æ¯”è¾ƒå·¥å…·è°ƒç”¨
        stream_tools = norm_stream.get('tool_calls', [])
        non_stream_tools = norm_non_stream.get('tool_calls', [])
        
        if len(stream_tools) != len(non_stream_tools):
            return False, f"ç¬¬{i+1}æ¡æ¶ˆæ¯å·¥å…·è°ƒç”¨æ•°é‡ä¸ä¸€è‡´: æµå¼={len(stream_tools)}, éæµå¼={len(non_stream_tools)}"
        
        for j, (stream_tool, non_stream_tool) in enumerate(zip(stream_tools, non_stream_tools)):
            if stream_tool.get('type') != non_stream_tool.get('type'):
                return False, f"ç¬¬{i+1}æ¡æ¶ˆæ¯ç¬¬{j+1}ä¸ªå·¥å…·è°ƒç”¨ç±»å‹ä¸ä¸€è‡´"
            
            stream_func = stream_tool.get('function', {})
            non_stream_func = non_stream_tool.get('function', {})
            
            if stream_func.get('name') != non_stream_func.get('name'):
                return False, f"ç¬¬{i+1}æ¡æ¶ˆæ¯ç¬¬{j+1}ä¸ªå·¥å…·è°ƒç”¨åç§°ä¸ä¸€è‡´"
            
            if stream_func.get('arguments') != non_stream_func.get('arguments'):
                return False, f"ç¬¬{i+1}æ¡æ¶ˆæ¯ç¬¬{j+1}ä¸ªå·¥å…·è°ƒç”¨å‚æ•°ä¸ä¸€è‡´"
    
    return True, "ä¸Šä¸‹æ–‡å®Œå…¨ä¸€è‡´"


async def test_context_consistency():
    """æµ‹è¯•æµå¼å’Œéæµå¼æ¨¡å¼çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æµå¼å’Œéæµå¼æ¨¡å¼çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§...")
    
    # å‡†å¤‡æµ‹è¯•å¯¹è¯
    test_prompts = [
        "è¯·å‘Šè¯‰æˆ‘åŒ—äº¬çš„å¤©æ°”å¦‚ä½•ï¼Ÿ",
        "ç°åœ¨è®¡ç®—ä¸€ä¸‹ 15 + 27 ç­‰äºå¤šå°‘ï¼Ÿ",
        "å†æŸ¥è¯¢ä¸€ä¸‹ä¸Šæµ·çš„å¤©æ°”ï¼Œç„¶åæŠŠä¸Šæµ·çš„æ¸©åº¦å’Œåˆšæ‰åŒ—äº¬çš„æ¸©åº¦ç›¸åŠ ã€‚"
    ]
    
    try:
        # æµ‹è¯•æµå¼æ¨¡å¼
        print("\n=== æµ‹è¯•æµå¼æ¨¡å¼ ===")
        stream_agent = await create_test_agent()
        stream_context = await run_conversation_stream(stream_agent, test_prompts)
        
        # æµ‹è¯•éæµå¼æ¨¡å¼
        print("\n=== æµ‹è¯•éæµå¼æ¨¡å¼ ===")
        non_stream_agent = await create_test_agent()
        non_stream_context = await run_conversation_non_stream(non_stream_agent, test_prompts)
        
        # æ‰“å°ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"\n=== ä¸Šä¸‹æ–‡åˆ†æ ===")
        print(f"æµå¼æ¨¡å¼æ¶ˆæ¯æ•°é‡: {len(stream_context)}")
        print(f"éæµå¼æ¨¡å¼æ¶ˆæ¯æ•°é‡: {len(non_stream_context)}")
        
        print(f"\næµå¼æ¨¡å¼ä¸Šä¸‹æ–‡æ¦‚è§ˆ:")
        for i, msg in enumerate(stream_context):
            role = msg.get('role', 'æœªçŸ¥')
            content_preview = (msg.get('content', '') or '')[:50] + "..." if len((msg.get('content', '') or '')) > 50 else (msg.get('content', '') or '')
            tool_calls_count = len(msg.get('tool_calls', []))
            print(f"  {i+1}. {role}: '{content_preview}' (å·¥å…·è°ƒç”¨: {tool_calls_count})")
        
        print(f"\néæµå¼æ¨¡å¼ä¸Šä¸‹æ–‡æ¦‚è§ˆ:")
        for i, msg in enumerate(non_stream_context):
            role = msg.get('role', 'æœªçŸ¥')
            content_preview = (msg.get('content', '') or '')[:50] + "..." if len((msg.get('content', '') or '')) > 50 else (msg.get('content', '') or '')
            tool_calls_count = len(msg.get('tool_calls', []))
            print(f"  {i+1}. {role}: '{content_preview}' (å·¥å…·è°ƒç”¨: {tool_calls_count})")
        
        # æ¯”è¾ƒä¸Šä¸‹æ–‡
        is_consistent, message = compare_contexts(stream_context, non_stream_context)
        
        print(f"\n=== æµ‹è¯•ç»“æœ ===")
        if is_consistent:
            print("âœ… æµ‹è¯•é€šè¿‡: æµå¼å’Œéæµå¼æ¨¡å¼çš„ä¸Šä¸‹æ–‡å®Œå…¨ä¸€è‡´!")
            print(f"è¯¦ç»†ä¿¡æ¯: {message}")
            return True
        else:
            print("âŒ æµ‹è¯•å¤±è´¥: æµå¼å’Œéæµå¼æ¨¡å¼çš„ä¸Šä¸‹æ–‡ä¸ä¸€è‡´!")
            print(f"å·®å¼‚: {message}")
            
            # è¾“å‡ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡å†…å®¹ç”¨äºè°ƒè¯•
            print(f"\n=== è¯¦ç»†å¯¹æ¯” ===")
            print("æµå¼æ¨¡å¼å®Œæ•´ä¸Šä¸‹æ–‡:")
            for i, msg in enumerate(stream_context):
                print(f"  [{i+1}] {msg}")
            
            print("\néæµå¼æ¨¡å¼å®Œæ•´ä¸Šä¸‹æ–‡:")
            for i, msg in enumerate(non_stream_context):
                print(f"  [{i+1}] {msg}")
            
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_basic_context_operations():
    """æµ‹è¯•åŸºç¡€çš„ä¸Šä¸‹æ–‡æ“ä½œ"""
    print("\nğŸ§ª æµ‹è¯•åŸºç¡€ä¸Šä¸‹æ–‡æ“ä½œ...")
    
    try:
        agent = await create_test_agent()
        
        # æ£€æŸ¥åˆå§‹çŠ¶æ€
        initial_messages = len(agent.context.messages)
        print(f"åˆå§‹æ¶ˆæ¯æ•°é‡: {initial_messages}")
        
        # æ·»åŠ ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        agent.context.add_user_message("æµ‹è¯•æ¶ˆæ¯")
        after_user = len(agent.context.messages)
        print(f"æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å: {after_user}")
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        openai_messages = agent.context.to_openai_messages()
        print(f"OpenAIæ ¼å¼æ¶ˆæ¯æ•°é‡: {len(openai_messages)}")
        
        # éªŒè¯æ¶ˆæ¯å†…å®¹
        user_msg = next((msg for msg in openai_messages if msg.get('role') == 'user'), None)
        if user_msg and user_msg.get('content') == 'æµ‹è¯•æ¶ˆæ¯':
            print("âœ… åŸºç¡€ä¸Šä¸‹æ–‡æ“ä½œæµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ åŸºç¡€ä¸Šä¸‹æ–‡æ“ä½œæµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ åŸºç¡€ä¸Šä¸‹æ–‡æ“ä½œæµ‹è¯•å‡ºé”™: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¤– å¼€å§‹ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æµ‹è¯•...")
    
    async def run_all_tests():
        results = []
        
        # åŸºç¡€ä¸Šä¸‹æ–‡æ“ä½œæµ‹è¯•
        basic_result = await test_basic_context_operations()
        results.append(("åŸºç¡€ä¸Šä¸‹æ–‡æ“ä½œ", basic_result))
        
        # ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æµ‹è¯•
        consistency_result = await test_context_consistency()
        results.append(("ä¸Šä¸‹æ–‡ä¸€è‡´æ€§", consistency_result))
        
        # ç»Ÿè®¡ç»“æœ
        passed = sum(1 for _, result in results if result)
        total = len(results)
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        for test_name, result in results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"  {test_name}: {status}")
        
        print(f"\næ€»ä½“ç»“æœ: {passed}/{total} é€šè¿‡")
        
        if passed == total:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†!")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šä¸‹æ–‡ç®¡ç†é€»è¾‘")
            
        return passed == total
    
    return asyncio.run(run_all_tests())


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 